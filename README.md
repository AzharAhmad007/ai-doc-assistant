# ğŸ¤– AI Document Assistant â€” Quick Rebuild Reference

A lightweight **AI-powered document assistant** built using **FastAPI + LangChain + Ollama + FAISS**.  
It allows you to upload documents, embed them, and query them using local LLMs.

---

## ğŸš€ Features

- ğŸ“„ Multi-document upload (PDF supported)
- ğŸ” Semantic search using FAISS vector store
- ğŸ§  Local LLM inference via Ollama
- âš¡ FastAPI backend with Swagger UI
- ğŸ” Easy environment rebuild using `requirements.txt`

---

## ğŸ› ï¸ Environment Setup

### 1ï¸âƒ£ Create project folder
```bash
mkdir ai-doc-assistant && cd ai-doc-assistant
2ï¸âƒ£ Create virtual environment
python3 -m venv venv
source venv/bin/activate        # Mac/Linux
3ï¸âƒ£ Upgrade pip
pip install --upgrade pip
ğŸ“¦ Install Dependencies
pip install fastapi uvicorn langchain langchain-community langchain-ollama faiss-cpu pypdf python-multipart

Save dependencies:

pip freeze > requirements.txt
ğŸ” Rebuild Later from requirements.txt

If you delete your environment or move to another machine:

python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
ğŸ§  Ollama Setup (One-Time System Install)

Install Ollama from:

https://ollama.com

Then run:

ollama pull llama3
ollama pull nomic-embed-text
ollama list

âœ”ï¸ This downloads:

llama3 â†’ LLM for answering questions

nomic-embed-text â†’ embeddings model for vector search

ğŸ“ Project Structure
ai-doc-assistant/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/          # uploaded documents
â”œâ”€â”€ indexes/       # FAISS vector indexes
â””â”€â”€ venv/

Create folders:

mkdir data indexes
â–¶ï¸ Run Server
python -m uvicorn main:app --reload --port 8001
